{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.ibm_adv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bdb8fe6d3b9fd5ad6031173592d6cfd3d0b2b010416a1a1b1405bb3204431f06"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "import nltk\n",
    "import wordcloud\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pyarrow\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "conf = SparkConf().setMaster(\"local[*]\") \\\n",
    "    .setAll([(\"spark.driver.memory\", \"16g\"),\\\n",
    "            (\"spark.executor.memory\", \"8g\"), \\\n",
    "            (\"spark.driver.maxResultSize\", \"16g\")])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.stop()\n",
    "#sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getElapsedTime(startTime, endTime):\n",
    "    elapsedTime = endTime - startTime\n",
    "    return(\"Process time = %.2f seconds.\"%(elapsedTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePandasDFToParquet(csvPath, parqPath, rawSchema, printTime=False):\n",
    "    startTime = time.time()\n",
    "    pandasDF = pd.read_csv(csvPath, header=None)\n",
    "    pandasDF.columns = rawSchema.names\n",
    "    pandasDF.to_parquet(parqPath, engine=\"pyarrow\")\n",
    "    endTime = time.time()\n",
    "    if printTime:\n",
    "        print(getElapsedTime(startTime=startTime, endTime=endTime))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSparkDFFromParquet(csvPath, parqPath, rawSchema, printTime=False):\n",
    "    parquetFile = Path(parqPath)\n",
    "    if (parquetFile.is_file() == False):\n",
    "        print(\"Parquet file not found... converting %s to parquet!\"%(csvPath))\n",
    "        savePandasDFToParquet(csvPath=csvPath, parqPath=parqPath, rawSchema=rawSchema, printTime=printTime)\n",
    "    sparkDF = spark.read.parquet(parqPath)\n",
    "    return (sparkDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawSchema = StructType([\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"review_heading\", StringType(), True),\n",
    "    StructField(\"review_text\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parquet file not found... converting data/train.csv to parquet!\n",
      "Process time = 26.17 seconds.\n",
      "Parquet file not found... converting data/test.csv to parquet!\n",
      "Process time = 5.56 seconds.\n",
      "+------+--------------------+--------------------+\n",
      "|rating|      review_heading|         review_text|\n",
      "+------+--------------------+--------------------+\n",
      "|     3|  more like funchuck|Gave this to my d...|\n",
      "|     5|           Inspiring|I hope a lot of p...|\n",
      "|     5|The best soundtra...|I'm reading a lot...|\n",
      "|     4|    Chrono Cross OST|The music of Yasu...|\n",
      "|     5| Too good to be true|Probably the grea...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Gave this to my dad for a gag gift after directing \"Nunsense,\" he got a reall kick out of it!\n"
     ]
    }
   ],
   "source": [
    "trainDF = readSparkDFFromParquet(csvPath=\"data/train.csv\", parqPath=\"data/train.parquet\", rawSchema=rawSchema, printTime=True)\n",
    "testDF = readSparkDFFromParquet(csvPath=\"data/test.csv\", parqPath=\"data/test.parquet\", rawSchema=rawSchema, printTime=True)\n",
    "trainDF.show(5)\n",
    "print(trainDF.take(1)[0][\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}