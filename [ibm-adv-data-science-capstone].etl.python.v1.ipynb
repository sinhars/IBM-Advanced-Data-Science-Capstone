{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('.ibm_adv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bdb8fe6d3b9fd5ad6031173592d6cfd3d0b2b010416a1a1b1405bb3204431f06"
    }
   }
  },
  "metadata": {
   "interpreter": {
    "hash": "f7177c0ae9b5f17be49d5fee0218316f97afdf27c50d7946af4bb7924b3b993c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IBM Advanced Data Science Capstone Project\n",
    "## Sentiment Analysis of Amazon Customer Reviews\n",
    "### Harsh V Singh, Apr 2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Extract, Transform, Load (ETL)\n",
    "\n",
    "This notebook contains the comprehensive step-by-step process used for cleaning and preparing the raw data. \n",
    "\n",
    "1. The data that we are using for this project is avaiable to us in the form of two csv files (train.csv/ test.csv). We will read these files into memory and then store them in parquet files with the same name. *Spark csv reader is not able to handle commas within the quoted text of the reviews. Hence, we will first read the files into Pandas dataframes and then export them into parquet files*.\n",
    "\n",
    "2. Since the training data is quite large, we will conduct the initial data exploration and analysis on a sample set of ~10,000 rows. Once we have finalized the ETL steps, we will implement them onto the entire train and test sets.\n",
    "\n",
    "3. As part of data exploration, we will look at the distribution of heading and review text lengths and number of words. We will also look at the most common words in the review texts, both for stopwords and other words.\n",
    "\n",
    "4. As part of data processing, we will use the **nltk** package to remove stopwords, clean and tokenize the text, and lemmatize the token words. \n",
    "\n",
    "5. Our **target variable** will be a transformation of the review ratings. Ratings above 3 (i.e. 4/5) will be categorized as positive while ratings below 3 will be categorized as negative. *For the purpose of sentiment analysis, we will ignore all reviews with rating 3 as their categorization is ambiguous*.\n",
    "\n",
    "6. Lastly, we will convert the tokenized word arrays into count-based and TFIDF-based sparse vectors which will be used as our final feature sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1. Importing required Python libraries and initializing Apache Spark environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pyarrow\n",
    "\n",
    "import string\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "ENGLISH_STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "from pyspark.sql.functions import udf, rand\n",
    "conf = SparkConf().setMaster(\"local[*]\") \\\n",
    "    .setAll([(\"spark.driver.memory\", \"16g\"),\\\n",
    "            (\"spark.executor.memory\", \"8g\"), \\\n",
    "            (\"spark.driver.maxResultSize\", \"16g\")])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "ETL_SAMPLE_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.stop()"
   ]
  },
  {
   "source": [
    "### 2. Reading data from CSV and storing parquet files\n",
    "\n",
    "The data that we are using for this project is avaiable to us in the form of two csv files (train.csv/ test.csv). We will read these files into memory and then store them in parquet files with the same name. \n",
    "\n",
    "We will write a function called **readSparkDFFromParquet** will read the parquet files into memory as Spark dataframes. In case the parquet files are not found, this function will call another function called **savePandasDFToParquet** which reads the original csv files into Pandas dataframe and saves them as **parquet** files.  \n",
    "\n",
    "*The reason why we need to read the csv files into a Pandas dataframe is bacause the Spark csv reader function is not able to handle commas within the quoted text of the reviews. In order to solve that, we will use the Pandas csv reader to process the data initially and then export them into parquet files*.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print time taken by a particular process, given the start and end times\n",
    "def getElapsedTime(startTime, endTime):\n",
    "    elapsedTime = endTime - startTime\n",
    "    return(\"Process time = %.2f seconds.\"%(elapsedTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema that defines the columns and datatypes of the data in the csv files\n",
    "rawSchema = StructType([\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"review_heading\", StringType(), True),\n",
    "    StructField(\"review_text\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a Pandas dataframe as a parquet file\n",
    "def savePandasDFToParquet(csvPath, parqPath, rawSchema, printTime=False):\n",
    "    startTime = time.time()\n",
    "    pandasDF = pd.read_csv(csvPath, header=None)\n",
    "    pandasDF.columns = rawSchema.names\n",
    "    pandasDF.to_parquet(parqPath, engine=\"pyarrow\")\n",
    "    endTime = time.time()\n",
    "    if printTime:\n",
    "        print(getElapsedTime(startTime=startTime, endTime=endTime))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a parquet file into a Spark dataframe\n",
    "# If the parquet file is not found, it will be created from the original csv\n",
    "def readSparkDFFromParquet(csvPath, parqPath, rawSchema, printTime=False):\n",
    "    parquetFile = Path(parqPath)\n",
    "    if (parquetFile.is_file() == False):\n",
    "        print(\"Parquet file not found... converting %s to parquet!\"%(csvPath))\n",
    "        savePandasDFToParquet(csvPath=csvPath, parqPath=parqPath, rawSchema=rawSchema, printTime=printTime)\n",
    "    sparkDF = spark.read.parquet(parqPath)\n",
    "    return (sparkDF)\n"
   ]
  },
  {
   "source": [
    "We will load the train and test sets and print a few samples as well as the size of the datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRaw = readSparkDFFromParquet(csvPath=\"data/train.csv\", parqPath=\"data/train.parquet\", rawSchema=rawSchema, printTime=True)\n",
    "testRaw = readSparkDFFromParquet(csvPath=\"data/test.csv\", parqPath=\"data/test.parquet\", rawSchema=rawSchema, printTime=True)\n",
    "trainRaw.show(5)\n",
    "print(\"There are %d/ %d samples in the training/ test data.\"%(trainRaw.count(), testRaw.count()))\n",
    "print(\"Sample review text: %s\"%(trainRaw.take(1)[0][\"review_text\"]))"
   ]
  },
  {
   "source": [
    "Since the training dataset is quite large, we will conduct the data exploration and basic analysis on a sample set, whose size is defined as a global variable, **ETL_SAMPLE_SIZE**. We will convert this to a Pandas dataframe for the analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleRaw = trainRaw.orderBy(rand()).limit(ETL_SAMPLE_SIZE).toPandas()\n",
    "sampleRaw.head()"
   ]
  },
  {
   "source": [
    "### 3. Data exploration\n",
    "\n",
    "We need to ensure that the entire training dataset consists only of **English** language reviews. We will use the **langdetect** package to check that and drop all the training data rows where language is not *en*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that call the detect function in langdetect package to predict the language of a given text\n",
    "def detectTextLanguage(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = \"error\"\n",
    "    return lang\n",
    "\n",
    "langDetectUDF = udf(lambda x: detectTextLanguage(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleRaw[\"lang\"] = sampleRaw.apply(lambda x: detectTextLanguage(x[\"review_text\"]), axis=1)\n",
    "sampleRaw.drop(sampleRaw[sampleRaw[\"lang\"] != \"en\"].index, inplace=True)\n",
    "sampleRaw.drop(columns=\"lang\", inplace=True)\n",
    "\n",
    "print(\"There are %d samples left after dropping non-english language reviews.\"%(sampleRaw.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that plots multiple customized histograms given certain datasets  \n",
    "def plotHistograms(datasets, titles, figTitle, figSize=(18,6), numCols=1):\n",
    "    fig = plt.figure(figsize=figSize)\n",
    "    sns.set_theme()\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    numRows = math.ceil(len(datasets) / numCols)\n",
    "    for i in range(len(datasets)):\n",
    "        fig.add_subplot(numRows, numCols, i+1)\n",
    "        sns.histplot(data=datasets[i])\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(titles[i])\n",
    "    \n",
    "    fig.suptitle(figTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "We will plot histograms in order to visualize the length of the review heading and text. Similarly, we will also look at the distribution for number of words in the headings and review texts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistograms(\n",
    "    datasets=[\n",
    "        sampleRaw['review_heading'].str.len(),\n",
    "        sampleRaw['review_text'].str.len()],\n",
    "    titles=[\"Review Headings\", \"Review Text\"],\n",
    "    figTitle=\"Distribution of String Lengths (Sample Data)\",\n",
    "    figSize=(18,6), numCols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistograms(\n",
    "    datasets=[\n",
    "        sampleRaw['review_heading'].str.split().map(lambda x: len(x)),\n",
    "        sampleRaw['review_text'].str.split().map(lambda x: len(x))\n",
    "        ],\n",
    "    titles=[\"Review Headings\", \"Review Text\"],\n",
    "    figTitle=\"Distribution of Word Counts (Sample Data)\",\n",
    "    figSize=(18,6), numCols=2\n",
    ")"
   ]
  },
  {
   "source": [
    "### 3. Data cleansing and pre-processing\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the top N most common words and their counts\n",
    "def getSortedWordCounts(wordCounts, topN=0):\n",
    "    sortedCounts = [[k, v] for k, v in sorted(wordCounts.items(), key=lambda item: -item[1])]\n",
    "    sortedCounts = pd.DataFrame(sortedCounts, columns = [\"word\", \"count\"]) \n",
    "    if(topN > 0):\n",
    "        sortedCounts = sortedCounts.head(min(topN, sortedCounts.shape[0]))\n",
    "    return (sortedCounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that uses word_tokenize from the nltk package to convert text strings into word tokens\n",
    "# This function also cleans the token words by removing any punctuation and only keeps words which contain alphabets \n",
    "def getWordTokensFromText(textData):\n",
    "    rawTokens = word_tokenize(textData)\n",
    "    cleanTokens = [w.lower().translate(str.maketrans('', '', string.punctuation)) for w in rawTokens]\n",
    "    wordList = [word for word in cleanTokens if word.isalpha()]\n",
    "    return (wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes as input a list of words and their counts to return the most common stopwords and other words in the list\n",
    "def getTopWords(wordList, stopWords, topN=25):\n",
    "    stopCounts = defaultdict(int)\n",
    "    otherCounts = defaultdict(int)\n",
    "    for word in wordList:\n",
    "        if word in stopWords:\n",
    "            stopCounts[word] += 1\n",
    "        else:\n",
    "            otherCounts[word] += 1\n",
    "\n",
    "    topStopWords = getSortedWordCounts(stopCounts, topN)\n",
    "    topOtherWords = getSortedWordCounts(otherCounts, topN)\n",
    "\n",
    "    return ({\"stopWords\": topStopWords, \"otherWords\": topOtherWords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the sample data to convert the text into tokenized word arrays\n",
    "sampleTokenized = sampleRaw.copy(deep=True)\n",
    "sampleTokenized[\"review_heading\"] = [getWordTokensFromText(text) for text in sampleTokenized[\"review_heading\"]]\n",
    "sampleTokenized[\"review_text\"] = [getWordTokensFromText(text) for text in sampleTokenized[\"review_text\"]]\n",
    "\n",
    "headingWords = sampleTokenized[\"review_heading\"].apply(pd.Series).stack().reset_index(drop = True).to_list()\n",
    "textWords = sampleTokenized[\"review_text\"].apply(pd.Series).stack().reset_index(drop = True).to_list()\n",
    "\n",
    "# Get the list fo most common heading/ text words (stopwords and others)\n",
    "topHeadingWords = getTopWords(wordList=headingWords, stopWords=ENGLISH_STOP_WORDS, topN=25)\n",
    "topTextWords = getTopWords(wordList=textWords, stopWords=ENGLISH_STOP_WORDS, topN=25)\n",
    "\n",
    "print(\"There are %d words in the review texts of %d samples.\"%(len(textWords), sampleTokenized.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that plots multiple custom horizontal bar plots\n",
    "def plotBars(datasets, titles, x, y, figTitle, figSize=(12,6), numCols=1):\n",
    "    fig = plt.figure(figsize=figSize)\n",
    "    sns.set_theme()\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    numRows = math.ceil(len(datasets) / numCols)\n",
    "    for i in range(len(datasets)):\n",
    "        fig.add_subplot(numRows, numCols, i+1)\n",
    "        sns.barplot(data=datasets[i], x=y, y=x)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(titles[i])\n",
    "    fig.suptitle(figTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBars(\n",
    "    datasets=[topHeadingWords[\"stopWords\"], topHeadingWords[\"otherWords\"], topTextWords[\"stopWords\"], topTextWords[\"otherWords\"]], \n",
    "    titles=[\"Headings - Stop Words\", \"Headings - Other Words\", \"Text - Stop Words\", \"Text - Other Words\"],\n",
    "    x=\"word\", y=\"count\", \n",
    "    figTitle=\"Count of Top Words in Headings and Review Texts (Sample Data)\", \n",
    "    figSize=(20,12), numCols=2)"
   ]
  },
  {
   "source": [
    "Next, we will process the tokenized data by combining the token words in the headings and review texts into a single column called **review_content**. We will also categorize all the data into positive/ negative sentiment reviews. All reviews with ratings more than 3 will be categorized as positive while all reviews with ratings less than 3 will be categorized as negative.\n",
    "\n",
    "*We will drop all the rows where review rating is 3 as they are ambiguous in terms of the customers sentiment.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleProcessed = sampleTokenized.copy()\n",
    "sampleProcessed[\"review_content\"] = sampleProcessed[\"review_heading\"] + sampleProcessed[\"review_text\"]\n",
    "sampleProcessed.loc[sampleProcessed[\"rating\"] < 3, \"review_sentiment\"] = 0\n",
    "sampleProcessed.loc[sampleProcessed[\"rating\"] > 3, \"review_sentiment\"] = 1\n",
    "sampleProcessed.drop(columns=[\"review_heading\", \"review_text\", \"rating\"], inplace=True)\n",
    "sampleProcessed.dropna(axis=0, inplace=True)\n",
    "sampleProcessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from a given array of words\n",
    "def removeStopWordsFromText(textData, stopWords):\n",
    "    relevantText = [word for word in textData if word not in stopWords]\n",
    "    return (relevantText)"
   ]
  },
  {
   "source": [
    "### 4. Dimensionality reduction by removing stop words and Lemmatization\n",
    "\n",
    "In order to reduce the dimensionality of the feature set and also reduce the noise in the training data, we will remove all the stop words from the review_content. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleProcessed[\"review_content\"] = [removeStopWordsFromText(text, ENGLISH_STOP_WORDS) for text in sampleProcessed[\"review_content\"]]\n",
    "sampleProcessed.head()"
   ]
  },
  {
   "source": [
    "Now that we have a list of relevant words from each review, we will convert these words into their respective *lemmas*. This process is known as **Lemmatisation** in linguistics and is the process of grouping together the inflected forms of a word so they can be analysed as a single item. For example, variations of the word run such as ran, running, runs, etc. will all be replaced with the dictionary form of the word, i.e., run. This will further help in reducing the dimensionality of the feature set.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the NOUN form of any given word, using wordnet data from the nltk package\n",
    "def getWordnetPos(word):\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tagDictionary = {\n",
    "      \"J\": wordnet.ADJ,\n",
    "      \"N\": wordnet.NOUN,\n",
    "      \"V\": wordnet.VERB,\n",
    "      \"R\": wordnet.ADV\n",
    "      }\n",
    "  return (tagDictionary.get(tag, wordnet.NOUN))\n",
    "\n",
    "# Function that returns the lemmatized version of the text, i.e. replacing each word with its lemma\n",
    "def getLemmatizedText(textData, lemmatizer):\n",
    "  lemText = [lemmatizer.lemmatize(word, getWordnetPos(word)) for word in textData]\n",
    "  return (lemText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "sampleProcessed[\"review_content\"] = [getLemmatizedText(text, lemmatizer) for text in sampleProcessed[\"review_content\"]]\n",
    "endTime = time.time()\n",
    "print(getElapsedTime(startTime=startTime, endTime=endTime))\n",
    "sampleProcessed.head()\n"
   ]
  },
  {
   "source": [
    "Finally, we can use the CountVectorizer function of the **scikit-learn** package to convert each review from a tokenized, lemmatized word array into a sparse vector containing the counts of each word in the review. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVect = CountVectorizer()\n",
    "reviewCounts = countVect.fit_transform(sampleProcessed[\"review_content\"].apply(\" \".join))\n",
    "print(\"Review content is transformed into a %s with %s elements.\"%(type(reviewCounts), reviewCounts.shape, ))"
   ]
  },
  {
   "source": [
    "### 5. Saving the preprocessed training and test data\n",
    "\n",
    "We will then split this sample data into training and test sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviewCounts, sampleProcessed[\"review_sentiment\"], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\", metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfVect = TfidfVectorizer()\n",
    "reviewTF = tfVect.fit_transform(sampleProcessed[\"review_content\"].apply(\" \".join))\n",
    "print(\"Review texts are transformed into a %s with %s elements.\"%(type(reviewTF), reviewTF.shape, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "trainClean = trainRaw.withColumn(\"lang\", langDetectFunc(\"review_text\"))\n",
    "trainClean = trainClean.filter(trainDF[\"lang\"] == \"en\")\n",
    "trainClean = trainClean.drop(\"lang\")\n",
    "trainClean.show(5)\n",
    "endTime = time.time()\n",
    "print(getElapsedTime(startTime=startTime, endTime=endTime))\n",
    "#print(\"There are %d samples left after dropping non-english language reviews.\"%(trainClean.count()))"
   ]
  }
 ]
}