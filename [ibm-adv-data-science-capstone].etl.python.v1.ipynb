{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0f7177c0ae9b5f17be49d5fee0218316f97afdf27c50d7946af4bb7924b3b993c",
   "display_name": "Python 3.9.1  ('.ibm_adv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f7177c0ae9b5f17be49d5fee0218316f97afdf27c50d7946af4bb7924b3b993c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IBM Advanced Data Science Capstone Project\n",
    "## Sentiment Analysis of Amazon Customer Reviews\n",
    "### Harsh V Singh, Apr 2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Extract, Transform, Load (ETL)\n",
    "\n",
    "This notebook contains the comprehensive step-by-step process for preparing the raw data to be used in the project. The data that we are using is avaiable in the form of two csv files (train.csv/ test.csv). We will read these files into memory and then store them in parquet files with the same name. *Spark csv reader is not able to handle commas within the quoted text of the reviews. Hence, we will first read the files into Pandas dataframes and then export them into parquet files*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing required Python libraries and initializing Apache Spark environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "conf = SparkConf().setMaster(\"local[*]\") \\\n",
    "    .setAll([(\"spark.driver.memory\", \"16g\"),\\\n",
    "             (\"spark.executor.memory\", \"4g\"), \\\n",
    "             (\"spark.driver.maxResultSize\", \"16g\"), \\\n",
    "             (\"spark.executor.cores\", \"4\")])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.stop()"
   ]
  },
  {
   "source": [
    "## Reading data from CSV and storing local copies\n",
    "\n",
    "The data that we are using for this project is avaiable to us in the form of two csv files (train.csv/ test.csv). We will read these files into memory and then store them in parquet files with the same name. \n",
    "\n",
    "We will write a function called **readSparkDFFromParquet** will read the parquet files into memory as Spark dataframes. In case the parquet files are not found, this function will call another function called **savePandasDFToParquet** which reads the original csv files into Pandas dataframe and saves them as **parquet** files.  \n",
    "\n",
    "*The reason why we need to read the csv files into a Pandas dataframe is bacause the Spark csv reader function is not able to handle commas within the quoted text of the reviews. In order to solve that, we will use the Pandas csv reader to process the data initially and then export them into parquet files*.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print time taken by a particular process, given the start and end times\n",
    "def printElapsedTime(startTime, endTime):\n",
    "    elapsedTime = endTime - startTime\n",
    "    print(\"Process time = %.2f seconds.\"%(elapsedTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema that defines the columns and datatypes of the data in the csv files\n",
    "rawSchema = StructType([\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"review_heading\", StringType(), True),\n",
    "    StructField(\"review_text\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a Pandas dataframe as a parquet file\n",
    "def savePandasDFToParquet(csvPath, parqPath, rawSchema, printTime=False):\n",
    "    startTime = time.time()\n",
    "    pandasDF = pd.read_csv(csvPath, header=None)\n",
    "    pandasDF.columns = rawSchema.names\n",
    "    pandasDF.to_parquet(parqPath, engine=\"pyarrow\")\n",
    "    endTime = time.time()\n",
    "    if printTime:\n",
    "        printElapsedTime(startTime=startTime, endTime=endTime)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a parquet file into a Spark dataframe\n",
    "# If the parquet file is not found, it will be created from the original csv\n",
    "def readSparkDFFromParquet(csvPath, parqPath, rawSchema, printTime=False):\n",
    "    parquetFile = Path(parqPath)\n",
    "    if (parquetFile.is_file() == False):\n",
    "        print(\"Parquet file not found... converting %s to parquet!\"%(csvPath))\n",
    "        savePandasDFToParquet(csvPath=csvPath, parqPath=parqPath, rawSchema=rawSchema, printTime=printTime)\n",
    "    sparkDF = spark.read.parquet(parqPath)\n",
    "    return (sparkDF)"
   ]
  },
  {
   "source": [
    "## Load local data for sanity check\n",
    "\n",
    "We will load the train and test sets and print a few samples as well as the size of the datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|rating|      review_heading|         review_text|\n",
      "+------+--------------------+--------------------+\n",
      "|     3|  more like funchuck|Gave this to my d...|\n",
      "|     5|           Inspiring|I hope a lot of p...|\n",
      "|     5|The best soundtra...|I'm reading a lot...|\n",
      "|     4|    Chrono Cross OST|The music of Yasu...|\n",
      "|     5| Too good to be true|Probably the grea...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "There are 3000000/ 650000 samples in the training/ test data.\n",
      "Sample review text: Gave this to my dad for a gag gift after directing \"Nunsense,\" he got a reall kick out of it!\n"
     ]
    }
   ],
   "source": [
    "trainRaw = readSparkDFFromParquet(csvPath=\"data/raw/train.csv\", parqPath=\"data/train.parquet\", rawSchema=rawSchema, printTime=True)\n",
    "testRaw = readSparkDFFromParquet(csvPath=\"data/raw/test.csv\", parqPath=\"data/test.parquet\", rawSchema=rawSchema, printTime=True)\n",
    "trainRaw.show(5)\n",
    "print(\"There are %d/ %d samples in the training/ test data.\"%(trainRaw.count(), testRaw.count()))\n",
    "print(\"Sample review text: %s\"%(trainRaw.take(1)[0][\"review_text\"]))"
   ]
  }
 ]
}