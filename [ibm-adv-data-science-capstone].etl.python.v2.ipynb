{"cells":[{"metadata":{},"cell_type":"markdown","source":["# IBM Advanced Data Science Capstone Project\n","## Sentiment Analysis of Amazon Customer Reviews\n","### Harsh V Singh, Apr 2021"]},{"metadata":{},"cell_type":"markdown","source":["## Extract, Transform, Load (ETL)\n","\n","This notebook contains the comprehensive step-by-step process for preparing the raw data to be used in the project. The data that we are using is avaiable in the form of two csv files (train.csv/ test.csv). We will read these files into memory and then store them in parquet files with the same name. *Spark csv reader is not able to handle commas within the quoted text of the reviews. Hence, we will first read the files into Pandas dataframes and then export them into parquet files*."]},{"metadata":{},"cell_type":"markdown","source":["## Importing required Python libraries and initializing Apache Spark environment"]},{"metadata":{},"cell_type":"code","source":["from ibm_botocore.client import Config\n","import ibm_boto3\n","\n","import pandas as pd\n","import csv\n","import time\n","from pathlib import Path\n","import shutil\n","\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SQLContext, SparkSession\n","from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, ArrayType\n","from pyspark.sql.functions import udf, rand, col, concat, coalesce\n","from pyspark.ml.feature import HashingTF, IDF, Word2Vec, Word2VecModel\n","\n","conf = SparkConf().setMaster(\"local[*]\")\n","sc = SparkContext.getOrCreate(conf=conf)\n","from pyspark.sql import SparkSession\n","spark = SparkSession \\\n","    .builder \\\n","    .getOrCreate()"],"execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# @hidden_cell\n","# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n","# You might want to remove those credentials before you share your notebook.\n","creds = {\n","    'IAM_SERVICE_ID': 'iam-ServiceId-a1e6ae17-a480-4a92-b3b8-b5927994ec39',\n","    'IBM_API_KEY_ID': '0bgYhpiB1he87QNeaFI-4_WD04bUy0JZZPTKR86Tyf-Z',\n","    'ENDPOINT': 'https://s3.eu.cloud-object-storage.appdomain.cloud',\n","    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n","    'BUCKET': 'ibmadvanceddatasciencecapstonepro-donotdelete-pr-nswuywrsyrm5si'\n","}\n","\n","#LAhA7x-MDHxGUxmx6ZY0CU8m-0zDmlNZJ2RT5kHwKfN8\n","\n","cos = ibm_boto3.client(service_name='s3',\n","    ibm_api_key_id=creds['IBM_API_KEY_ID'],\n","    ibm_service_instance_id=creds['IAM_SERVICE_ID'],\n","    ibm_auth_endpoint=creds['IBM_AUTH_ENDPOINT'],\n","    config=Config(signature_version='oauth'),\n","    endpoint_url=creds['ENDPOINT'])"],"execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Reading data from CSV and storing local copies\n","\n","The data that we are using for this project is avaiable to us in the form of two csv files (train.csv/ test.csv). We will read these files into memory and then store them in parquet files with the same name. \n","\n","We will write a function called **readSparkDFFromParquet** will read the parquet files into memory as Spark dataframes. In case the parquet files are not found, this function will call another function called **savePandasDFToParquet** which reads the original csv files into Pandas dataframe and saves them as **parquet** files.  \n","\n","*The reason why we need to read the csv files into a Pandas dataframe is bacause the Spark csv reader function is not able to handle commas within the quoted text of the reviews. In order to solve that, we will use the Pandas csv reader to process the data initially and then export them into parquet files*.\n"]},{"metadata":{},"cell_type":"code","source":["# Function to print time taken by a particular process, given the start and end times\n","def printElapsedTime(startTime, endTime):\n","    elapsedTime = endTime - startTime\n","    print(\"-- Process time = %.2f seconds --\"%(elapsedTime))"],"execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Schema that defines the columns and datatypes of the data in the csv files\n","rawSchema = StructType([\n","    StructField(\"rating\", IntegerType(), True),\n","    StructField(\"review_heading\", StringType(), True),\n","    StructField(\"review_text\", StringType(), True)\n","    ])"],"execution_count":5,"outputs":[]},{"source":["## Download raw CSV files and upload converted parquet files\n","\n","We will first check if the IBM Cloud Storage bucket contains the converted parquet files. If not, we will download the CSV data and then convert them to parquet files using Spark. Finally, we will upload these parquet files to the cloud storage."],"cell_type":"markdown","metadata":{}},{"metadata":{},"cell_type":"code","source":["# Function to save a Pandas dataframe as a parquet file\n","def saveCSVToParquet(creds, cos, csvFile, parqPath, rawSchema, printTime=False):\n","    startTime = time.time()\n","    # Read csv to pandas dataframe\n","    cos.download_file(Bucket=creds[\"BUCKET\"], Key=csvFile, Filename=csvFile)\n","    pandasDF = pd.read_csv(csvFile, header=None)\n","    pandasDF.columns = rawSchema.names\n","    # Convert pandas to spark dataframe\n","    parquetDF = spark.createDataFrame(pandasDF, schema=rawSchema)\n","    parquetDF.write.mode(\"overwrite\").parquet(parqPath)\n","    shutil.make_archive(parqPath, 'tar', parqPath)\n","    \n","    # Upload parquet file to COS\n","    cos.upload_file(Filename=parqPath+\".tar\", Bucket=creds[\"BUCKET\"], Key=parqPath)\n","    endTime = time.time()\n","    if printTime:\n","        printElapsedTime(startTime=startTime, endTime=endTime)\n","    return"],"execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Fetch existing files from the COS bucket\n","cosBucketContent = cos.list_objects(Bucket=creds[\"BUCKET\"])[\"Contents\"]\n","cosFileNames = [x[\"Key\"] for x in cosBucketContent]\n","\n","# Convert CSV to parquet and upload to COS if files don't exist\n","if \"trainRaw.parquet\" not in cosFileNames:\n","    saveCSVToParquet(cos=cos, creds=creds, csvFile=\"train.csv\", parqPath=\"trainRaw.parquet\", rawSchema=rawSchema, printTime=True)\n","if \"testRaw.parquet\" not in cosFileNames:\n","    saveCSVToParquet(cos=cos, creds=creds, csvFile=\"test.csv\", parqPath=\"testRaw.parquet\", rawSchema=rawSchema, printTime=True)"],"execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Load local data for sanity check\n","\n","We will load the train and test sets and print a few samples as well as the size of the datasets."]},{"metadata":{},"cell_type":"code","source":["# Function to read a parquet file into a Spark dataframe\n","# If the parquet file is not found, it will be created from the original csv\n","def readParquetToSparkDF(creds, cos, parqPath, rawSchema):\n","    cos.download_file(Bucket=creds[\"BUCKET\"], Key=parqPath, Filename=parqPath+\".tar\")\n","    shutil.unpack_archive(parqPath+\".tar\", parqPath, \"tar\")\n","    parquetDF = spark.read.schema(rawSchema).parquet(parqPath)\n","    return (parquetDF)"],"execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"code","source":["# Load train and test parquet dataframes from IBM Cloud Storage\n","trainRaw = readParquetToSparkDF(cos=cos, creds=creds, parqPath=\"trainRaw.parquet\", rawSchema=rawSchema)\n","testRaw = readParquetToSparkDF(cos=cos, creds=creds, parqPath=\"testRaw.parquet\", rawSchema=rawSchema)\n","\n","trainRaw.show(5)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------------+--------------------+\n|rating|      review_heading|         review_text|\n+------+--------------------+--------------------+\n|     3|  more like funchuck|Gave this to my d...|\n|     5|           Inspiring|I hope a lot of p...|\n|     5|The best soundtra...|I'm reading a lot...|\n|     4|    Chrono Cross OST|The music of Yasu...|\n|     5| Too good to be true|Probably the grea...|\n+------+--------------------+--------------------+\nonly showing top 5 rows\n\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"python391jvsc74a57bd0bdb8fe6d3b9fd5ad6031173592d6cfd3d0b2b010416a1a1b1405bb3204431f06","display_name":"Python 3.9.1 64-bit ('.ibm_adv')"},"metadata":{"interpreter":{"hash":"f7177c0ae9b5f17be49d5fee0218316f97afdf27c50d7946af4bb7924b3b993c"}},"language_info":{"name":"python","version":"3.9.1","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}