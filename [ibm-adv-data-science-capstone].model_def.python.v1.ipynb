{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0bdb8fe6d3b9fd5ad6031173592d6cfd3d0b2b010416a1a1b1405bb3204431f06",
   "display_name": "Python 3.9.1  ('.ibm_adv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "bdb8fe6d3b9fd5ad6031173592d6cfd3d0b2b010416a1a1b1405bb3204431f06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IBM Advanced Data Science Capstone Project\n",
    "## Sentiment Analysis of Amazon Customer Reviews\n",
    "### Harsh V Singh, Apr 2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model Definition\n",
    "\n",
    "In this notebook, we will define the machine learning model that will be used to train and predict the sentiment of an Amazon customer's review given its review heading and text. We have already preprocessed the raw data into a training set containing tokenized and vectorized features of the review text content along with a binary review sentiment which is 1 for positive and 0 for negative reviews.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing required Python libraries and initializing Apache Spark environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, ArrayType\n",
    "from pyspark.sql.functions import udf, rand, col, concat, coalesce\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "CPU_CORES = 6\n",
    "conf = SparkConf().setMaster(\"local[*]\") \\\n",
    "    .setAll([(\"spark.driver.memory\", \"24g\"),\\\n",
    "             (\"spark.executor.memory\", \"4g\"), \\\n",
    "             (\"spark.driver.maxResultSize\", \"24g\"), \\\n",
    "             (\"spark.executor.cores\", CPU_CORES), \\\n",
    "             (\"spark.executor.heartbeatInterval\", \"3600s\"), \\\n",
    "             (\"spark.network.timeout\", \"7200s\")])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SEED_NUMBER = 1324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print time taken by a particular process, given the start and end times\n",
    "def printElapsedTime(startTime, endTime):\n",
    "    elapsedTime = endTime - startTime\n",
    "    print(\"Process time = %.2f seconds.\"%(elapsedTime))"
   ]
  },
  {
   "source": [
    "## Loading data\n",
    "\n",
    "We will begin by loading the train/ test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDir = \"data/sample\"\n",
    "X_train = sparse.load_npz(sourceDir + \"/X_train.npz\")\n",
    "X_test = sparse.load_npz(sourceDir + \"/X_test.npz\")\n",
    "y_train = pd.read_csv(sourceDir + \"/y_train.csv\")[\"review_sentiment\"].array\n",
    "y_test = pd.read_csv(sourceDir + \"/y_test.csv\")[\"review_sentiment\"].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MultinomialNB Accuracy: 82.38%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy: %.2f%%\"%(100 * metrics.accuracy_score(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> b05ed88f0cfee7a939376131ad9c9c3118290bf0
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}