{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0f7177c0ae9b5f17be49d5fee0218316f97afdf27c50d7946af4bb7924b3b993c",
   "display_name": "Python 3.9.1  ('.ibm_adv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f7177c0ae9b5f17be49d5fee0218316f97afdf27c50d7946af4bb7924b3b993c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IBM Advanced Data Science Capstone Project\n",
    "## Sentiment Analysis of Amazon Customer Reviews\n",
    "### Harsh V Singh, Apr 2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This notebook contains the comprehensive step-by-step process used for cleaning and preparing the raw data. \n",
    "\n",
    "1. Since the training data is quite large, we will conduct the initial data exploration and analysis on a sample set of ~10,000 rows. Once we have finalized the ETL steps, we will implement them onto the entire train and test sets.\n",
    "\n",
    "2. As part of data exploration, we will look at the distribution of heading and review text lengths and number of words. We will also look at the most common words in the review texts, both for stopwords and other words.\n",
    "\n",
    "3. As part of data processing, we will use the **nltk** package to remove stopwords, clean and tokenize the text, and lemmatize the token words. \n",
    "\n",
    "4. Our **target variable** will be a transformation of the review ratings. Ratings above 3 (i.e. 4/5) will be categorized as positive while ratings below 3 will be categorized as negative. *For the purpose of sentiment analysis, we will ignore all reviews with rating 3 as their categorization is ambiguous*.\n",
    "\n",
    "5. Lastly, we will convert the tokenized word arrays into TFIDF-based sparse vectors which will be used as our final feature sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing required Python libraries and initializing Apache Spark environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import string\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "ENGLISH_STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, ArrayType\n",
    "from pyspark.sql.functions import udf, rand, col, concat, coalesce\n",
    "from pyspark.ml.feature import HashingTF, IDF, Word2Vec, Word2VecModel\n",
    "\n",
    "CPU_CORES = 4\n",
    "conf = SparkConf().setMaster(\"local[*]\") \\\n",
    "    .setAll([(\"spark.driver.memory\", \"24g\"),\\\n",
    "             (\"spark.executor.memory\", \"24g\"), \\\n",
    "             (\"spark.driver.maxResultSize\", \"24g\"), \\\n",
    "             (\"spark.executor.cores\", CPU_CORES), \\\n",
    "             (\"spark.executor.heartbeatInterval\", \"3600s\"), \\\n",
    "             (\"spark.network.timeout\", \"7200s\")])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SAMPLE_SIZE = 500000 # Subset of training data will be used for exploration\n",
    "ROWS_PER_PARTITION = 10000\n",
    "SEED_NUMBER = 1324\n",
    "RUN_SAMPLE_CODE = False\n",
    "RUN_SPARK_CODE = True\n",
    "SAVE_OUTPUT = True\n",
    "NUM_FEATURES = 2**15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print time taken by a particular process, given the start and end times\n",
    "def printElapsedTime(startTime, endTime):\n",
    "    elapsedTime = endTime - startTime\n",
    "    print(\"-- Process time = %.2f seconds --\"%(elapsedTime))"
   ]
  },
  {
   "source": [
    "## Loading raw data\n",
    "\n",
    "We will begin the data preprocessing by loading the raw data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema that defines the columns and datatypes of the data in the csv files\n",
    "rawSchema = StructType([\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"review_heading\", StringType(), True),\n",
    "    StructField(\"review_text\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a parquet file into a Spark dataframe\n",
    "# If the parquet file is not found, it will be created from the original csv\n",
    "def readSparkDFFromParquet(csvPath, parqPath, rawSchema, printTime=False):\n",
    "    parquetFile = Path(parqPath)\n",
    "    if (parquetFile.is_file() == False):\n",
    "        print(\"Parquet file not found... converting %s to parquet!\"%(csvPath))\n",
    "        savePandasDFToParquet(csvPath=csvPath, parqPath=parqPath, rawSchema=rawSchema, printTime=printTime)\n",
    "    sparkDF = spark.read.parquet(parqPath)\n",
    "    return (sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRaw = readSparkDFFromParquet(csvPath=\"data/raw/train.csv\", parqPath=\"data/train.parquet\", rawSchema=rawSchema, printTime=True)\n",
    "trainRaw = trainRaw.orderBy(rand(seed=SEED_NUMBER)).limit(SAMPLE_SIZE)\n",
    "rowCount = trainRaw.count()\n",
    "numPartitions = math.ceil(rowCount / ROWS_PER_PARTITION)\n",
    "trainRaw = trainRaw.repartition(numPartitions)\n",
    "print(\"There are %d row in the training data partitioned into %d parts.\"%(rowCount, numPartitions))"
   ]
  },
  {
   "source": [
    "Since the training dataset is quite large, we will conduct the data exploration and analysis on a sample set, whose size is defined as a global variable, **ETL_SAMPLE_SIZE**. We will convert this to a Pandas dataframe for the analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    sampleRaw = trainRaw.limit(SAMPLE_SIZE).toPandas()\n",
    "    print(\"There are %d samples in the training data.\"%(sampleRaw.shape[0]))"
   ]
  },
  {
   "source": [
    "## Data exploration\n",
    "\n",
    "First, we should find and remove any rows with null values in the sample data. We also need to ensure that the entire training dataset consists only of **English** language reviews. We will use the **langdetect** package to check that and drop all the training data rows where language is not *en*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    sampleRaw.dropna(axis=0, inplace=True)\n",
    "    print(\"After removing empty/ null values, there are %d samples in the training data.\"%(sampleRaw.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that call the detect function in langdetect package to predict the language of a given text\n",
    "def detectTextLanguage(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = \"error\"\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    startTime = time.time()\n",
    "    sampleRaw[\"lang\"] = sampleRaw.apply(lambda x: detectTextLanguage(x[\"review_text\"]), axis=1)\n",
    "    sampleRaw.drop(sampleRaw[sampleRaw[\"lang\"] != \"en\"].index, inplace=True)\n",
    "    sampleRaw.drop(columns=\"lang\", inplace=True)\n",
    "\n",
    "    print(\"There are %d samples left after dropping non-english language reviews.\"%(sampleRaw.shape[0]))\n",
    "    endTime = time.time()\n",
    "    printElapsedTime(startTime=startTime, endTime=endTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that plots multiple customized histograms given certain datasets  \n",
    "def plotHistograms(datasets, titles, figTitle, figSize=(18,6), numCols=1, binWidth=1):\n",
    "    fig = plt.figure(figsize=figSize)\n",
    "    sns.set_theme()\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    numRows = math.ceil(len(datasets) / numCols)\n",
    "    for i in range(len(datasets)):\n",
    "        fig.add_subplot(numRows, numCols, i+1)\n",
    "        sns.histplot(data=datasets[i], binwidth=binWidth)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(titles[i])\n",
    "    \n",
    "    fig.suptitle(figTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "We will plot histograms in order to visualize the length of the review heading and text. Similarly, we will also look at the distribution for number of words in the headings and review texts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    plotHistograms(\n",
    "        datasets=[\n",
    "            sampleRaw['review_heading'].str.len(),\n",
    "            sampleRaw['review_text'].str.len()],\n",
    "        titles=[\"Review Headings\", \"Review Text\"],\n",
    "        figTitle=\"Distribution of String Lengths (Sample Data)\",\n",
    "        figSize=(18,6), numCols=2, binWidth=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    plotHistograms(\n",
    "        datasets=[\n",
    "            sampleRaw['review_heading'].str.split().map(lambda x: len(x)),\n",
    "            sampleRaw['review_text'].str.split().map(lambda x: len(x))\n",
    "            ],\n",
    "        titles=[\"Review Headings\", \"Review Text\"],\n",
    "        figTitle=\"Distribution of Word Counts (Sample Data)\",\n",
    "        figSize=(18,6), numCols=2, binWidth=1\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Data cleansing and pre-processing\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the top N most common words and their counts\n",
    "def getSortedWordCounts(wordCounts, topN=0):\n",
    "    sortedCounts = [[k, v] for k, v in sorted(wordCounts.items(), key=lambda item: -item[1])]\n",
    "    sortedCounts = pd.DataFrame(sortedCounts, columns = [\"word\", \"count\"]) \n",
    "    if(topN > 0):\n",
    "        sortedCounts = sortedCounts.head(min(topN, sortedCounts.shape[0]))\n",
    "    return (sortedCounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that uses word_tokenize from the nltk package to convert text strings into word tokens\n",
    "# This function also cleans the token words by removing any punctuation and only keeps words which contain alphabets \n",
    "def getWordTokensFromText(textData):\n",
    "    rawTokens = word_tokenize(textData)\n",
    "    cleanTokens = [w.lower().translate(str.maketrans('', '', string.punctuation)) for w in rawTokens]\n",
    "    wordList = [word for word in cleanTokens if word.isalpha()]\n",
    "    return (wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes as input a list of words and their counts to return the most common stopwords and other words in the list\n",
    "def getTopWords(wordList, stopWords, topN=25):\n",
    "    stopCounts = defaultdict(int)\n",
    "    otherCounts = defaultdict(int)\n",
    "    for word in wordList:\n",
    "        if word in stopWords:\n",
    "            stopCounts[word] += 1\n",
    "        else:\n",
    "            otherCounts[word] += 1\n",
    "\n",
    "    topStopWords = getSortedWordCounts(stopCounts, topN)\n",
    "    topOtherWords = getSortedWordCounts(otherCounts, topN)\n",
    "\n",
    "    return ({\"stopWords\": topStopWords, \"otherWords\": topOtherWords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the sample data to convert the text into tokenized word arrays\n",
    "if RUN_SAMPLE_CODE:\n",
    "    sampleTokenized = sampleRaw.copy(deep=True)\n",
    "    sampleTokenized[\"review_heading\"] = [getWordTokensFromText(text) for text in sampleTokenized[\"review_heading\"]]\n",
    "    sampleTokenized[\"review_text\"] = [getWordTokensFromText(text) for text in sampleTokenized[\"review_text\"]]\n",
    "\n",
    "    headingWords = sampleTokenized[\"review_heading\"].apply(pd.Series).stack().reset_index(drop = True).to_list()\n",
    "    textWords = sampleTokenized[\"review_text\"].apply(pd.Series).stack().reset_index(drop = True).to_list()\n",
    "\n",
    "    # Get the list fo most common heading/ text words (stopwords and others)\n",
    "    topHeadingWords = getTopWords(wordList=headingWords, stopWords=ENGLISH_STOP_WORDS, topN=25)\n",
    "    topTextWords = getTopWords(wordList=textWords, stopWords=ENGLISH_STOP_WORDS, topN=25)\n",
    "\n",
    "    print(\"There are %d words in the review texts of %d samples.\"%(len(textWords), sampleTokenized.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that plots multiple custom horizontal bar plots\n",
    "def plotBars(datasets, titles, x, y, figTitle, figSize=(12,6), numCols=1):\n",
    "    fig = plt.figure(figsize=figSize)\n",
    "    sns.set_theme()\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    numRows = math.ceil(len(datasets) / numCols)\n",
    "    for i in range(len(datasets)):\n",
    "        fig.add_subplot(numRows, numCols, i+1)\n",
    "        sns.barplot(data=datasets[i], x=y, y=x)\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.title(titles[i])\n",
    "    fig.suptitle(figTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    plotBars(\n",
    "        datasets=[topHeadingWords[\"stopWords\"], topHeadingWords[\"otherWords\"], topTextWords[\"stopWords\"], topTextWords[\"otherWords\"]], \n",
    "        titles=[\"Headings - Stop Words\", \"Headings - Other Words\", \"Text - Stop Words\", \"Text - Other Words\"],\n",
    "        x=\"word\", y=\"count\", \n",
    "        figTitle=\"Count of Top Words in Headings and Review Texts (Sample Data)\", \n",
    "        figSize=(20,12), numCols=2)"
   ]
  },
  {
   "source": [
    "We can visualize the words present in the reviews by creating a word cloud using the **WordCloud** package."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "# TODO: Add code to draw a word cloud"
   ]
  },
  {
   "source": [
    "Next, we will process the tokenized data by combining the token words in the headings and review texts into a single column called **review_content**. We will also categorize all the data into positive/ negative sentiment reviews. All reviews with ratings more than 3 will be categorized as positive while all reviews with ratings less than 3 will be categorized as negative.\n",
    "\n",
    "*We will drop all the rows where review rating is 3 as they are ambiguous in terms of the customers sentiment.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReviewSentiment(rating):\n",
    "    sentiment = None\n",
    "    if(rating > 3):\n",
    "        sentiment = 1\n",
    "    elif(rating < 3):\n",
    "        sentiment = 0\n",
    "    return (sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    sampleProcessed = sampleTokenized.copy()\n",
    "    sampleProcessed[\"review_content\"] = sampleProcessed[\"review_heading\"] + sampleProcessed[\"review_text\"]\n",
    "    sampleProcessed[\"review_sentiment\"] = [getReviewSentiment(rating) for rating in sampleProcessed[\"rating\"]]\n",
    "    \n",
    "    sampleProcessed.drop(columns=[\"review_heading\", \"review_text\", \"rating\"], inplace=True)\n",
    "    sampleProcessed.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "source": [
    "## Dimensionality reduction - Removing stop words + Lemmatization\n",
    "\n",
    "In order to reduce the dimensionality of the feature set and also reduce the noise in the training data, we will remove all the stop words from **review_content**. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from a given array of words\n",
    "def removeStopWordsFromText(textData, stopWords):\n",
    "    relevantText = [word for word in textData if word not in stopWords]\n",
    "    return (relevantText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    sampleProcessed[\"review_content\"] = [removeStopWordsFromText(text, ENGLISH_STOP_WORDS) for text in sampleProcessed[\"review_content\"]]"
   ]
  },
  {
   "source": [
    "Now that we have a list of relevant words from each review, we will convert these words into their respective *lemmas*. This process is known as **Lemmatisation** in linguistics and is the process of grouping together the inflected forms of a word so they can be analysed as a single item. For example, variations of the word run such as ran, running, runs, etc. will all be replaced with the dictionary form of the word, i.e., run. This will further help in reducing the dimensionality of the feature set.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the NOUN form of any given word, using wordnet data from the nltk package\n",
    "def getWordnetPos(word):\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tagDictionary = {\n",
    "      \"J\": \"a\", #wordnet.ADJ,\n",
    "      \"N\": \"n\", #wordnet.NOUN,\n",
    "      \"V\": \"v\", #wordnet.VERB,\n",
    "      \"R\": \"r\", #wordnet.ADV\n",
    "      }\n",
    "  return (tagDictionary.get(tag, \"n\")) #wordnet.NOUN))\n",
    "\n",
    "# Function that returns the lemmatized version of the text, i.e. replacing each word with its lemma\n",
    "def getLemmatizedText(textData, lemmatizer):\n",
    "  lemText = [lemmatizer.lemmatize(word, getWordnetPos(word)) for word in textData]\n",
    "  return (lemText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    startTime = time.time()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    sampleProcessed[\"review_content\"] = [getLemmatizedText(text, lemmatizer) for text in sampleProcessed[\"review_content\"]]\n",
    "    endTime = time.time()\n",
    "    printElapsedTime(startTime=startTime, endTime=endTime)"
   ]
  },
  {
   "source": [
    "### Method 1: Vectorized sparse matrix as features\n",
    "\n",
    "In the first method, we will vectorize the tokens into a sparse matrix using TFIDF Vectorizer from the **scikit-learn** package. This will create a list of features in the form of a sparse array containing the relative count of each word in a particular review. In this method, the sequential ordering of words is lost."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    tfVect = TfidfVectorizer()\n",
    "    tfData = tfVect.fit_transform(sampleProcessed[\"review_content\"].apply(\" \".join))\n",
    "    print(\"Review texts are transformed into a %s with %s elements.\"%(type(tfData), tfData.shape, ))"
   ]
  },
  {
   "source": [
    "We will split this data into training and test sets and save them to disk."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tfData, sampleProcessed[\"review_sentiment\"], test_size=0.2, random_state=SEED_NUMBER)\n",
    "\n",
    "    if SAVE_OUTPUT:\n",
    "        Path(\"data/sample/tfData\").mkdir(parents=True, exist_ok=True)\n",
    "        sparse.save_npz(\"data/sample/tfData/X_train.npz\", X_train)\n",
    "        sparse.save_npz(\"data/sample/tfData/X_test.npz\", X_test)\n",
    "        y_train.to_csv(\"data/sample/tfData/y_train.csv\", index=False)\n",
    "        y_test.to_csv(\"data/sample/tfData/y_test.csv\", index=False)"
   ]
  },
  {
   "source": [
    "### Method 2: Padded sequential word vectors as features\n",
    "\n",
    "In this method, we will simply convert the review text string into a sequenced array with padding added to make all review vectors of the same length. We will use the **tensorflow** package for this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaddedFeatureVectors(textData, tokenizer, maxSeqLength):\n",
    "    # Convert list of words to list of ints\n",
    "    sequence = tokenizer.texts_to_sequences([textData])\n",
    "    # Make all sequences of the same length by padding zeros\n",
    "    paddedSeq = pad_sequences(sequence, maxlen=maxSeqLength, padding=\"pre\")[0]\n",
    "    paddedSeq = [x.item() for x in paddedSeq]\n",
    "    return (paddedSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    # Tokenize the review_content using tensorflow Tokenizer\n",
    "    sampleTokenizer = Tokenizer(num_words=NUM_FEATURES)\n",
    "    sampleTokenizer.fit_on_texts(sampleProcessed[\"review_content\"].apply(lambda x: \" \".join(x)))\n",
    "    # Get the length of the longest sequence and add padding to the other vectors accordingly\n",
    "    maxSeqLength = max(len(s) for s in sampleProcessed[\"review_content\"])\n",
    "    seqData = sampleProcessed[\"review_content\"].apply(lambda x: getPaddedFeatureVectors(x, sampleTokenizer, maxSeqLength))"
   ]
  },
  {
   "source": [
    "We will split this data into training and test sets and save them to disk."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(seqData, sampleProcessed[\"review_sentiment\"], test_size=0.2, random_state=SEED_NUMBER)\n",
    "\n",
    "    if SAVE_OUTPUT:\n",
    "        Path(\"data/sample/seqData\").mkdir(parents=True, exist_ok=True)\n",
    "        np.save(\"data/sample/seqData/X_train.npy\", X_train)\n",
    "        np.save(\"data/sample/seqData/X_test.npy\", X_test)\n",
    "        y_train.to_csv(\"data/sample/seqData/y_train.csv\", index=False)\n",
    "        y_test.to_csv(\"data/sample/seqData/y_test.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Preprocessing training set using Spark\n",
    "\n",
    "Now that we have completed the preprocessing of the sample data, we will apply the same steps to the entire dataset. Since we are dealing with a relatively large dataset of ~3M rows, we will use Spark in order to reduce the time required to prepare the training data.\n",
    "\n",
    "### Cleaning and preprocessing text reviews -\n",
    "\n",
    "1. Remove null/ NaN rows from the dataset\n",
    "2. Transform ratings into a binary column by assigning 0 (negative) to *rating < 3* and 1 (positive) to *rating > 3*\n",
    "3. Drop all rows where *rating = 3*\n",
    "4. Remove non-English language reviews\n",
    "5. Remove punctuations and tokenize the words in review headings and text\n",
    "6. Combine heading and text tokens into a single column\n",
    "7. Remove stop words from tokenized words\n",
    "8. Lemmatize the remaining words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining User-defined functions for Spark \n",
    "\n",
    "langDetectUDF = udf(lambda x: detectTextLanguage(x), StringType())\n",
    "\n",
    "wordTokensUDF = udf(lambda x: getWordTokensFromText(x), ArrayType(StringType()))\n",
    "\n",
    "def removeStopWordsUDF(stopWords):\n",
    "    return udf(lambda x: removeStopWordsFromText(x, stopWords), ArrayType(StringType()))\n",
    "\n",
    "def lemmaTextUDF(lemmatizer):\n",
    "    return udf(lambda x: getLemmatizedText(x, lemmatizer), ArrayType(StringType()))\n",
    "\n",
    "reviewSentimentUDF = udf(lambda x: getReviewSentiment(x), IntegerType())\n",
    "\n",
    "def paddedVectorsUDF(tokenizer, maxSeqLength):\n",
    "    return udf(lambda x: getPaddedFeatureVectors(x, tokenizer, maxSeqLength), ArrayType(IntegerType()))\n",
    "\n",
    "getArrayLength = udf(lambda x: len(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SPARK_CODE:\n",
    "\n",
    "    startTime = time.time()\n",
    "    # Remove null/ NaN rows\n",
    "    print(\"-- Removing null/ NaN rows --\")\n",
    "    trainClean = trainRaw.dropna()\n",
    "\n",
    "    # Transform rating into positive/ negative sentiment\n",
    "    print(\"-- Transforming rating into positive/ negative sentiment --\")\n",
    "    trainClean = trainClean.withColumn(\"review_sentiment\", reviewSentimentUDF(\"rating\"))\n",
    "\n",
    "    # Drop all rows with sentiment = None\n",
    "    print(\"-- Dropping all rows with sentiment = None --\")\n",
    "    trainClean = trainClean.filter(trainClean[\"review_sentiment\"].isNotNull())\n",
    "\n",
    "    # Remove non-English reviews\n",
    "    print(\"-- Removing non-English reviews --\")\n",
    "    trainClean = trainClean.withColumn(\"lang\", langDetectUDF(\"review_text\"))\n",
    "    trainClean = trainClean.filter(trainClean[\"lang\"] == \"en\")\n",
    "    trainClean = trainClean.drop(\"lang\")\n",
    "\n",
    "    # Remove punctuations and tokenize words in review_heading and review_text\n",
    "    print(\"-- Removing punctuations and tokenize words in review_heading and review_text --\")\n",
    "    trainClean = trainClean.withColumn(\"token_heading\", wordTokensUDF(\"review_heading\"))\n",
    "    trainClean = trainClean.withColumn(\"token_text\", wordTokensUDF(\"review_text\"))\n",
    "\n",
    "    # Combine heading and text into single column\n",
    "    print(\"-- Combining heading and text into single column --\")\n",
    "    trainClean = trainClean.withColumn(\"token_content\", concat(col(\"token_heading\"), col(\"token_text\")))\n",
    "\n",
    "    # Remove all stop words \n",
    "    print(\"-- Removing all stop words --\")\n",
    "    trainClean = trainClean.withColumn(\"token_content_no_stops\", removeStopWordsUDF(ENGLISH_STOP_WORDS)(col(\"token_content\")))\n",
    "\n",
    "    # Lemmatize remaining words \n",
    "    print(\"-- Lemmatizing remaining words --\")\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    trainClean = trainClean.withColumn(\"token_content_lemma\", lemmaTextUDF(lemmatizer)(col(\"token_content_no_stops\")))\n",
    "\n",
    "    endTime = time.time()\n",
    "    print(\"-- Preprocessing lazy execution done --\")\n",
    "    printElapsedTime(startTime=startTime, endTime=endTime)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write trainData to disk\n",
    "if RUN_SPARK_CODE:\n",
    "    try:\n",
    "        startTime = time.time()\n",
    "        if SAVE_OUTPUT:\n",
    "            trainClean.write.parquet(\"data/trainClean.parquet\", mode=\"overwrite\")\n",
    "        endTime = time.time()\n",
    "        print(\"-- Actual execution done --\")\n",
    "        printElapsedTime(startTime=startTime, endTime=endTime)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "source": [
    "### Method 1: Sparse matrix as features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the lemmatized words into a sparse featureset\n",
    "if False:\n",
    "    \n",
    "    print(\"Vectorizing the lemmatized words into a sparse featureset\")\n",
    "    startTime = time.time()\n",
    "\n",
    "    hashingTF = HashingTF(inputCol=\"token_content_lemma\", outputCol=\"raw_features\", numFeatures=NUM_FEATURES)\n",
    "    featurizedData = hashingTF.transform(trainClean)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "    trainTF = rescaledData.select(\"features\", \"review_sentiment\")\n",
    "    if SAVE_OUTPUT:\n",
    "        trainTF.write.parquet(\"data/trainTF.parquet\", mode=\"overwrite\")\n",
    "\n",
    "    endTime = time.time()\n",
    "    print(\"-- Saved training data --\")\n",
    "    printElapsedTime(startTime=startTime, endTime=endTime)"
   ]
  },
  {
   "source": [
    "### Method 2: Padded vectors as features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the lemmatized words into padded integer vectors\n",
    "if RUN_SPARK_CODE:\n",
    "    \n",
    "    print(\"Vectorizing the lemmatized words into padded vector features\")\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # Train a Keras Tokenizer model that will replace each word in our data with a integer\n",
    "    finalTokenizer = Tokenizer(num_words=NUM_FEATURES)\n",
    "    contentDF = trainClean.select(\"token_content_lemma\").toPandas()\n",
    "    finalTokenizer.fit_on_texts(contentDF[\"token_content_lemma\"].apply(lambda x: \" \".join(x)))\n",
    "    \n",
    "    # Get the length of the longest sentence/ review in the data\n",
    "    maxSeqLength = trainClean.withColumn(\"array_len\", getArrayLength(col(\"token_content_lemma\"))).groupby().max(\"array_len\").collect()[0][0]\n",
    "    \n",
    "    trainSeq = trainClean.withColumn(\"features\", paddedVectorsUDF(finalTokenizer, maxSeqLength)(col(\"token_content_lemma\")))\n",
    "    trainSeq = trainSeq.select(\"features\", \"review_sentiment\")\n",
    "\n",
    "    if SAVE_OUTPUT:\n",
    "        with open(\"data/vectorTokenizer.pickle\", \"wb\") as handle:\n",
    "            pickle.dump(finalTokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        trainSeq.write.parquet(\"data/trainSeq.parquet\", mode=\"overwrite\")\n",
    "\n",
    "    endTime = time.time()\n",
    "    print(\"-- Saved training data --\")\n",
    "    printElapsedTime(startTime=startTime, endTime=endTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.stop()"
   ]
  }
 ]
}