{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0bdb8fe6d3b9fd5ad6031173592d6cfd3d0b2b010416a1a1b1405bb3204431f06",
   "display_name": "Python 3.9.1  ('.ibm_adv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "bdb8fe6d3b9fd5ad6031173592d6cfd3d0b2b010416a1a1b1405bb3204431f06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IBM Advanced Data Science Capstone Project\n",
    "## Sentiment Analysis of Amazon Customer Reviews\n",
    "### Harsh V Singh, Apr 2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model Definition\n",
    "\n",
    "In this notebook, we will define the machine learning model that will be used to train and predict the sentiment of an Amazon customer's review given its review heading and text. We have already preprocessed the raw data into a training set containing tokenized and vectorized features of the review text content along with a binary review sentiment which is 1 for positive and 0 for negative reviews."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing required Python libraries and initializing Apache Spark environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Masking, Embedding\n",
    "from keras import regularizers\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType, StringType, ArrayType\n",
    "from pyspark.sql.functions import udf, rand, col, concat, coalesce\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "CPU_CORES = 4\n",
    "conf = SparkConf().setMaster(\"local[*]\") \\\n",
    "    .setAll([(\"spark.driver.memory\", \"24g\"),\\\n",
    "             (\"spark.executor.memory\", \"8g\"), \\\n",
    "             (\"spark.driver.maxResultSize\", \"24g\"), \\\n",
    "             (\"spark.executor.cores\", CPU_CORES), \\\n",
    "             (\"spark.executor.heartbeatInterval\", \"3600s\"), \\\n",
    "             (\"spark.network.timeout\", \"7200s\")])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RUN_SAMPLE_CODE = True\n",
    "TRAIN_FINAL_MODEL = False\n",
    "SEED_NUMBER = 1324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print time taken by a particular process, given the start and end times\n",
    "def printElapsedTime(startTime, endTime):\n",
    "    elapsedTime = endTime - startTime\n",
    "    print(\"-- Process time = %.2f seconds --\"%(elapsedTime))"
   ]
  },
  {
   "source": [
    "## Method 1: Training models using TFIDF vectorized data\n",
    "\n",
    "First, we will use the TFIDF vectorized data to build a baseline Naive Bayes model and then train a neural network with 2 hidden layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading TFIDF train/ test data\n",
    "\n",
    "We will begin by loading the train/ test data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    sourceDir = \"data/sample/tfData\"\n",
    "    X_train_tf = sparse.load_npz(sourceDir + \"/X_train.npz\")\n",
    "    X_test_tf = sparse.load_npz(sourceDir + \"/X_test.npz\")\n",
    "\n",
    "    X_train_tf.sort_indices()\n",
    "    X_test_tf.sort_indices()\n",
    "\n",
    "    y_train_tf = pd.read_csv(sourceDir + \"/y_train.csv\")[\"review_sentiment\"].to_numpy()\n",
    "    y_test_tf = pd.read_csv(sourceDir + \"/y_test.csv\")[\"review_sentiment\"].to_numpy()\n",
    "\n",
    "    print(\"X_train_tf is of type %s and shape %s.\"%(type(X_train_tf), X_train_tf.shape))\n",
    "    print(\"y_train_tf is of type %s, shape %s and %d unique classes.\"%(type(y_train_tf), y_train_tf.shape, len(np.unique(y_train_tf))))"
   ]
  },
  {
   "source": [
    "### Predictions using a Naive Bayes model for setting a baseline\n",
    "\n",
    "**ComplementNB** implements the Complement Naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. CNB regularly outperforms MNB on text classification tasks so we will be using this model for our baseline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    tfCNBModel = ComplementNB().fit(X_train_tf, y_train_tf)\n",
    "    print(\"ComplementNB Accuracy: %.2f%%\"%(100 * metrics.accuracy_score(y_test_tf, tfCNBModel.predict(X_test_tf))))"
   ]
  },
  {
   "source": [
    "### Predictions using a Keras Neural Network\n",
    "\n",
    "We will be using a **Sequential** model with **two** hidden layers and a **sigmoid** activation for the output layer. We can experiment with the hyperparameters such as *L2 regularization, dropout rate, number of nodes in the hidden layers and the activation functions* to find the best possible combination that gives the best accuracy on the test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model accuracy and loss over the training epochs\n",
    "def plotTrainingPerformance(history, figTitle, figSize=(12,5)):\n",
    "    fig = plt.figure(figsize=figSize)\n",
    "    sns.set_theme()\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    metrics = history.model.metrics_names\n",
    "    xvals = np.arange(len(history.history[metrics[0]])) + 1\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "        fig.add_subplot(1, len(metrics), i + 1)\n",
    "        sns.lineplot(x=xvals, y=history.history[metrics[i]])\n",
    "        sns.lineplot(x=xvals, y=history.history[\"val_\" + metrics[i]])\n",
    "        plt.xticks(xvals)\n",
    "        plt.ylabel(metrics[i])\n",
    "    \n",
    "    fig.suptitle(figTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compile, fit and predict keras model\n",
    "def fitAndPredictModel(modelName, model, X_train, y_train, X_test, y_test, loss, optimizer, metrics, validationSplit, epochs, batch_size):\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    # Split training data into training/ validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train.reshape(-1,1), test_size=validationSplit, shuffle= True)\n",
    "    # Fit the model on the training data\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, y_valid))\n",
    "    # Plot training performance\n",
    "    plotTrainingPerformance(history=history, figTitle=\"Accuracy/ Loss over Epochs\")\n",
    "    # Predict review sentiments on the test data and check model accuracy\n",
    "    _, accuracy = model.evaluate(X_test, y_test.reshape((-1,1)))\n",
    "    print(\"%s Accuracy: %.2f%%\" % (modelName, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "\n",
    "    # Model definition\n",
    "    tfModel = Sequential()\n",
    "    l2Reg = 1e-3\n",
    "    dropout = 0.3\n",
    "    tfModel.add(Dense(256, input_shape=(X_train_tf.shape[1],), \\\n",
    "        kernel_regularizer=regularizers.l2(l2Reg), \\\n",
    "        bias_regularizer=regularizers.l2(l2Reg)))\n",
    "    tfModel.add(Activation('relu'))\n",
    "    tfModel.add(Dropout(dropout))\n",
    "    tfModel.add(Dense(256, input_shape=(X_train_tf.shape[1],), \\\n",
    "        kernel_regularizer=regularizers.l2(l2Reg), \\\n",
    "        bias_regularizer=regularizers.l2(l2Reg)))\n",
    "    tfModel.add(Activation('relu'))\n",
    "    tfModel.add(Dropout(dropout))\n",
    "    tfModel.add(Dense(1))\n",
    "    tfModel.add(Activation('sigmoid'))\n",
    "\n",
    "    print(tfModel.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    # Compile, fit and predict model\n",
    "    fitAndPredictModel(\n",
    "        modelName=\"Neural Network\", model=tfModel, X_train=X_train_tf, y_train=y_train_tf, X_test=X_test_tf, y_test=y_test_tf, \n",
    "        loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_accuracy\"], validationSplit=0.2, epochs=3, batch_size=64)"
   ]
  },
  {
   "source": [
    "## Method 2: Training models using sequential word vectors data\n",
    "\n",
    "Now, we will use the sequential word vectors data to train a recurrent neural network with **1 LSTM layer**, **1 Dense layer** and a **sigmoid** output layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading sample sequential train/ test data\n",
    "\n",
    "We will begin by loading the train/ test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    sourceDir = \"data/sample/seqData\"\n",
    "    X_train_seq = np.load(sourceDir + \"/X_train.npy\")\n",
    "    X_test_seq = np.load(sourceDir + \"/X_test.npy\")\n",
    "    y_train_seq = pd.read_csv(sourceDir + \"/y_train.csv\")[\"review_sentiment\"].to_numpy()\n",
    "    y_test_seq = pd.read_csv(sourceDir + \"/y_test.csv\")[\"review_sentiment\"].to_numpy()\n",
    "\n",
    "    vocabCount = (max([max(x) for x in X_train_seq]) + 1)"
   ]
  },
  {
   "source": [
    "### Predictions using a Keras LSTM Recurrent Neural Network\n",
    "\n",
    "We will be using a **Sequential** model with **one** .... and a **sigmoid** activation for the output layer. We can experiment with the hyperparameters such as *L2 regularization, dropout rate, number of nodes in the hidden layers and the activation functions* to find the best possible combination that gives the best accuracy on the test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    # Model definition\n",
    "    seqModel = Sequential()\n",
    "    l2Reg = 1e-2\n",
    "    dropout = 0.5\n",
    "    seqModel.add(Embedding(input_dim=vocabCount, output_dim=64, input_length=X_train_seq.shape[1], mask_zero=True))\n",
    "    seqModel.add(LSTM(128, return_sequences=False, dropout=dropout, recurrent_dropout=dropout))\n",
    "    seqModel.add(Dense(64, activation='relu', \\\n",
    "        kernel_regularizer=regularizers.l2(l2Reg), \\\n",
    "        bias_regularizer=regularizers.l2(l2Reg)))\n",
    "    seqModel.add(Dropout(dropout))\n",
    "    seqModel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(seqModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile, fit and predict model\n",
    "if RUN_SAMPLE_CODE:\n",
    "    fitAndPredictModel(\n",
    "        modelName=\"LSTM RNN\", model=seqModel, X_train=X_train_seq, y_train=y_train_seq, X_test=X_test_seq, y_test=y_test_seq, \n",
    "        loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"binary_accuracy\"], validationSplit=0.2, epochs=3, batch_size=64)"
   ]
  },
  {
   "source": [
    "## Final Model - Keras LSTM Model with Padded Sequential Feature Vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_FINAL_MODEL:\n",
    "    trainSeq = spark.read.parquet(\"data/trainSeq.parquet\")\n",
    "    print(\"There are %d samples in the training data.\"%(trainSeq.count()))\n",
    "    trainSeq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_FINAL_MODEL:\n",
    "    df = trainSeq.toPandas()\n",
    "    X_train = pd.DataFrame(df[\"features\"].to_list())\n",
    "    y_train = df[\"review_sentiment\"].to_numpy()\n",
    "    vocabCount = X_train.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_FINAL_MODEL:\n",
    "    finModel = Sequential()\n",
    "    l2Reg = 1e-2\n",
    "    dropout = 0.5\n",
    "    finModel.add(Embedding(input_dim=vocabCount, output_dim=64, input_length=X_train.shape[1], mask_zero=True))\n",
    "    finModel.add(LSTM(128, return_sequences=False, dropout=dropout, recurrent_dropout=dropout))\n",
    "    finModel.add(Dense(64, activation='relu', \\\n",
    "        kernel_regularizer=regularizers.l2(l2Reg), \\\n",
    "        bias_regularizer=regularizers.l2(l2Reg)))\n",
    "    finModel.add(Dropout(dropout))\n",
    "    finModel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(finModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_FINAL_MODEL:\n",
    "        fitAndPredictModel(\n",
    "                modelName=\"LSTM RNN\", model=finModel, X_train=X_train, y_train=y_train, X_test=X_train, y_test=y_train, \n",
    "                loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"], validationSplit=0.2, epochs=3, batch_size=64)\n"
   ]
  }
 ]
}