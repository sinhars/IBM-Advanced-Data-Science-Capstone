{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0f7177c0ae9b5f17be49d5fee0218316f97afdf27c50d7946af4bb7924b3b993c",
   "display_name": "Python 3.9.1 64-bit ('.ibm_adv': venv)",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "bdb8fe6d3b9fd5ad6031173592d6cfd3d0b2b010416a1a1b1405bb3204431f06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IBM Advanced Data Science Capstone Project\n",
    "## Sentiment Analysis of Amazon Customer Reviews\n",
    "### Harsh V Singh, Apr 2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model Definition\n",
    "\n",
    "In this notebook, we will define the machine learning model that will be used to train and predict the sentiment of an Amazon customer's review given its review heading and text. We have already preprocessed the raw data into a training set containing tokenized and vectorized features of the review text content along with a binary review sentiment which is 1 for positive and 0 for negative reviews."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing required Python libraries and initializing Apache Spark environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Masking, Embedding\n",
    "from keras import regularizers\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType, StringType, ArrayType\n",
    "from pyspark.sql.functions import udf, rand, col, concat, coalesce\n",
    "from pyspark.ml.feature import HashingTF, IDF, Word2VecModel\n",
    "\n",
    "CPU_CORES = 6\n",
    "conf = SparkConf().setMaster(\"local[*]\") \\\n",
    "    .setAll([(\"spark.driver.memory\", \"24g\"),\\\n",
    "             (\"spark.executor.memory\", \"4g\"), \\\n",
    "             (\"spark.driver.maxResultSize\", \"24g\"), \\\n",
    "             (\"spark.executor.cores\", CPU_CORES), \\\n",
    "             (\"spark.executor.heartbeatInterval\", \"3600s\"), \\\n",
    "             (\"spark.network.timeout\", \"7200s\")])\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "RUN_SAMPLE_CODE = True\n",
    "SEED_NUMBER = 1324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print time taken by a particular process, given the start and end times\n",
    "def printElapsedTime(startTime, endTime):\n",
    "    elapsedTime = endTime - startTime\n",
    "    print(\"-- Process time = %.2f seconds --\"%(elapsedTime))"
   ]
  },
  {
   "source": [
    "## Method 1: Training models using TFIDF vectorized data\n",
    "\n",
    "First, we will use the TFIDF vectorized data to build a baseline Naive Bayes model and then train a neural network with 2 hidden layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading TFIDF train/ test data\n",
    "\n",
    "We will begin by loading the train/ test data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train_tf is of type <class 'scipy.sparse.csr.csr_matrix'> and shape (31761, 75372).\ny_train_tf is of type <class 'numpy.ndarray'>, shape (31761,) and 2 unique classes.\n"
     ]
    }
   ],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    sourceDir = \"data/sample/tfData\"\n",
    "    X_train_tf = sparse.load_npz(sourceDir + \"/X_train.npz\")\n",
    "    X_test_tf = sparse.load_npz(sourceDir + \"/X_test.npz\")\n",
    "\n",
    "    X_train_tf.sort_indices()\n",
    "    X_test_tf.sort_indices()\n",
    "\n",
    "    y_train_tf = pd.read_csv(sourceDir + \"/y_train.csv\")[\"review_sentiment\"].to_numpy()\n",
    "    y_test_tf = pd.read_csv(sourceDir + \"/y_test.csv\")[\"review_sentiment\"].to_numpy()\n",
    "\n",
    "    print(\"X_train_tf is of type %s and shape %s.\"%(type(X_train_tf), X_train_tf.shape))\n",
    "    print(\"y_train_tf is of type %s, shape %s and %d unique classes.\"%(type(y_train_tf), y_train_tf.shape, len(np.unique(y_train_tf))))"
   ]
  },
  {
   "source": [
    "### Predictions using a Naive Bayes model for setting a baseline\n",
    "\n",
    "**ComplementNB** implements the Complement Naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. CNB regularly outperforms MNB on text classification tasks so we will be using this model for our baseline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ComplementNB Accuracy: 80.81%\n"
     ]
    }
   ],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    tfCNBModel = ComplementNB().fit(X_train_tf, y_train_tf)\n",
    "    print(\"ComplementNB Accuracy: %.2f%%\"%(100 * metrics.accuracy_score(y_test_tf, tfCNBModel.predict(X_test_tf))))"
   ]
  },
  {
   "source": [
    "### Predictions using a Keras Neural Network\n",
    "\n",
    "We will be using a **Sequential** model with **two** hidden layers and a **sigmoid** activation for the output layer. We can experiment with the hyperparameters such as *L2 regularization, dropout rate, number of nodes in the hidden layers and the activation functions* to find the best possible combination that gives the best accuracy on the test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model accuracy and loss over the training epochs\n",
    "def plotTrainingPerformance(history, figTitle, figSize=(12,5)):\n",
    "    fig = plt.figure(figsize=figSize)\n",
    "    sns.set_theme()\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    xvals = np.arange(len(history.history[\"accuracy\"])) + 1\n",
    "\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x=xvals, y=history.history[\"accuracy\"])\n",
    "    plt.xticks(xvals)\n",
    "    plt.ylabel(\"accuracy\")\n",
    "        \n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    sns.lineplot(x=xvals, y=history.history[\"loss\"])\n",
    "    plt.xticks(xvals)\n",
    "    plt.ylabel(\"loss\")\n",
    "    \n",
    "    fig.suptitle(figTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compile, fit and predict keras model\n",
    "def fitAndPredictModel(modelName, model, X_train, y_train, X_test, y_test, loss, optimizer, metrics, epochs, batch_size):\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    # Fit the model on the training data\n",
    "    history = model.fit(X_train, y_train.reshape((-1,1)), epochs=epochs, batch_size=batch_size)\n",
    "    # Plot training performance\n",
    "    plotTrainingPerformance(history=history, figTitle=\"Training Accuracy/ Loss over Epochs\")\n",
    "    # Predict review sentiments on the test data and check model accuracy\n",
    "    _, accuracy = model.evaluate(X_test, y_test.reshape((-1,1)))\n",
    "    print(\"%s Accuracy: %.2f%%\" % (modelName, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "\n",
    "    # Model definition\n",
    "    tfModel = Sequential()\n",
    "    l2Reg = 1e-3\n",
    "    dropout = 0.5\n",
    "    tfModel.add(Dense(32, input_shape=(X_train_tf.shape[1],), \\\n",
    "        kernel_regularizer=regularizers.l2(l2Reg), \\\n",
    "        bias_regularizer=regularizers.l2(l2Reg)))\n",
    "    tfModel.add(Activation('relu'))\n",
    "    tfModel.add(Dropout(dropout))\n",
    "    tfModel.add(Dense(32, input_shape=(X_train_tf.shape[1],), \\\n",
    "        kernel_regularizer=regularizers.l2(l2Reg), \\\n",
    "        bias_regularizer=regularizers.l2(l2Reg)))\n",
    "    tfModel.add(Activation('relu'))\n",
    "    tfModel.add(Dropout(dropout))\n",
    "    tfModel.add(Dense(1))\n",
    "    tfModel.add(Activation('sigmoid'))\n",
    "\n",
    "    print(tfModel.summary())\n",
    "\n",
    "    # Complie, fit and predict model\n",
    "    fitAndPredictModel(\n",
    "        modelName=\"Neural Network\", model=tfModel, X_train=X_train_tf, y_train=y_train_tf, X_test=X_test_tf, y_test=y_test_tf, \n",
    "        loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"], epochs=5, batch_size=128)"
   ]
  },
  {
   "source": [
    "## Method 2: Training models using sequential word vectors data\n",
    "\n",
    "Now, we will use the sequential word vectors data to train a recurrent neural network with **1 LSTM layer**, **1 Dense layer** and a **sigmoid** output layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading sample sequential train/ test data\n",
    "\n",
    "We will begin by loading the train/ test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    sourceDir = \"data/sample/seqData\"\n",
    "    X_train_seq = np.load(sourceDir + \"/X_train.npy\")\n",
    "    X_test_seq = np.load(sourceDir + \"/X_test.npy\")\n",
    "    y_train_seq = pd.read_csv(sourceDir + \"/y_train.csv\")[\"review_sentiment\"].to_numpy()\n",
    "    y_test_seq = pd.read_csv(sourceDir + \"/y_test.csv\")[\"review_sentiment\"].to_numpy()"
   ]
  },
  {
   "source": [
    "### Predictions using a Keras LSTM Recurrent Neural Network\n",
    "\n",
    "We will be using a **Sequential** model with **one** .... and a **sigmoid** activation for the output layer. We can experiment with the hyperparameters such as *L2 regularization, dropout rate, number of nodes in the hidden layers and the activation functions* to find the best possible combination that gives the best accuracy on the test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 107, 64)           2097152   \n_________________________________________________________________\nlstm (LSTM)                  (None, 32)                12416     \n_________________________________________________________________\ndense (Dense)                (None, 32)                1056      \n_________________________________________________________________\ndropout (Dropout)            (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 2,110,657\nTrainable params: 2,110,657\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "if RUN_SAMPLE_CODE:\n",
    "    vocabCount = (max([max(x) for x in X_train_seq]) + 1) # Max integer value in the training data + 1\n",
    "    l2Reg = 1e-2\n",
    "    dropout = 0.5\n",
    "\n",
    "    # Model definition\n",
    "    seqModel = Sequential()\n",
    "    seqModel.add(Embedding(input_dim=vocabCount, output_dim=64, input_length=X_train_seq.shape[1], mask_zero=True))\n",
    "    seqModel.add(LSTM(32, return_sequences=False, dropout=dropout, recurrent_dropout=dropout))\n",
    "    seqModel.add(Dense(32, activation='relu', \\\n",
    "        kernel_regularizer=regularizers.l2(l2Reg), \\\n",
    "        bias_regularizer=regularizers.l2(l2Reg)))\n",
    "    seqModel.add(Dropout(dropout))\n",
    "    seqModel.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(seqModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "WARNING: AutoGraph could not transform <bound method LSTMCell._compute_carry_and_output of <keras.layers.recurrent.LSTMCell object at 0x00000185DB06A310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmp7yv9o0cd.py, line 13)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x00000185DB141A90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmp803iz79z.py, line 48)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "249/249 [==============================] - 42s 98ms/step - loss: 0.7771 - accuracy: 0.7007\n",
      "Epoch 2/5\n",
      "249/249 [==============================] - 24s 97ms/step - loss: 0.3354 - accuracy: 0.8899\n",
      "Epoch 3/5\n",
      "222/249 [=========================>....] - ETA: 2s - loss: 0.2426 - accuracy: 0.9231"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-82d87b134ceb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Complie, fit and predict model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m fitAndPredictModel(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodelName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"LSTM RNN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseqModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test_seq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"], epochs=5, batch_size=128)\n",
      "\u001b[1;32m<ipython-input-6-fe61ed3557cf>\u001b[0m in \u001b[0;36mfitAndPredictModel\u001b[1;34m(modelName, model, X_train, y_train, X_test, y_test, loss, optimizer, metrics, epochs, batch_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Fit the model on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Plot training performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplotTrainingPerformance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigTitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training Accuracy/ Loss over Epochs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\Python\\IBM-Advanced-Data-Science-Capstone\\.ibm_adv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 _r=1):\n\u001b[0;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\Python\\IBM-Advanced-Data-Science-Capstone\\.ibm_adv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\Python\\IBM-Advanced-Data-Science-Capstone\\.ibm_adv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    898\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\Python\\IBM-Advanced-Data-Science-Capstone\\.ibm_adv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\Python\\IBM-Advanced-Data-Science-Capstone\\.ibm_adv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32md:\\Projects\\Python\\IBM-Advanced-Data-Science-Capstone\\.ibm_adv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\Python\\IBM-Advanced-Data-Science-Capstone\\.ibm_adv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complie, fit and predict model\n",
    "fitAndPredictModel(\n",
    "    modelName=\"LSTM RNN\", model=seqModel, X_train=X_train_seq, y_train=y_train_seq, X_test=X_test_seq, y_test=y_test_seq, \n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"], epochs=5, batch_size=128)"
   ]
  },
  {
   "source": [
    "## Final Model: Word2Vec-based Feature Vectors in a Keras Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqTrain = spark.read.parquet(\"data/seqTrain.parquet\")\n",
    "seqTrain = seqTrain.repartition(CPU_CORES)\n",
    "print(\"There are %d samples in the training data.\"%(seqTrain.count()))\n",
    "seqTrain.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2VecModel = Word2VecModel.load(\"data/word2Vec\")\n",
    "vocabWords = word2VecModel.getVectors()\n",
    "vocabSize = vocabWords.count()\n",
    "featureSize = word2VecModel.getVectorSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = seqTrain.toPandas()\n",
    "X = pd.DataFrame(df[\"features\"].apply(lambda x: x.toArray()).tolist())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df[\"review_sentiment\"], test_size=0.2, random_state=SEED_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(featureSize,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie, fit and predict model\n",
    "fitAndPredictModel(\n",
    "    modelName=\"Word2Vec Neural Network\", model=model, X_train=X_train, y_train=y_train.to_numpy(), \n",
    "    X_test=X_test, y_test=y_test.to_numpy(), loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"], \n",
    "    epochs=16, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}